{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 - The Agent-Environment Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridworld - Random Policy\n",
    "\n",
    "Here we replicate figure 3.5 (pg. 67) of Sutton-Barto, shown below.\n",
    "\n",
    "\n",
    "The system dynamics are fairly straightforward. You are in a square as your current state and you can move up, down, left, or right over a uniform probability distribution (e.g. the probability of a move in any individual direction is 0.25). If you move off the grid you receive a reward of -1 and your next state is your current state. If you find yourself in states A or B you will move deterministically to A' or B', respectively, and receive a reward of 10 or 5, respectively. Any other move in any other direction has a reward of 0. We discount the moves by a factor of 0.9. \n",
    "\n",
    "Graphically we show the reward dynamics and random state value functions below:\n",
    "\n",
    "![alt text](pic1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "    \n",
    "#Define system dynamics\n",
    "def movestate(inputstate, nextstate):\n",
    "    if inputstate == [0,1]:\n",
    "        reward = 10\n",
    "        prob = .25\n",
    "        outstate = [4,1]\n",
    "    elif inputstate == [0,3]:\n",
    "        reward = 5\n",
    "        prob = .25\n",
    "        outstate = [2,3]\n",
    "    elif nextstate[0] > 4 or nextstate[0] < 0 or nextstate[1] > 4 or nextstate[1] < 0:\n",
    "        outstate = inputstate\n",
    "        prob = .25\n",
    "        reward = -1\n",
    "    else:\n",
    "        outstate = nextstate\n",
    "        prob = .25\n",
    "        reward = 0\n",
    "    return outstate, prob, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.3  8.8  4.4  5.3  1.5]\n",
      " [ 1.5  3.   2.3  1.9  0.5]\n",
      " [ 0.1  0.7  0.7  0.4 -0.4]\n",
      " [-1.  -0.4 -0.4 -0.6 -1.2]\n",
      " [-1.9 -1.3 -1.2 -1.4 -2. ]]\n"
     ]
    }
   ],
   "source": [
    "grid = np.zeros((5,5))   \n",
    "\n",
    "#loop until convergence\n",
    "while True:\n",
    "    \n",
    "    newgrid = np.zeros((5,5))  \n",
    "\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            #Up\n",
    "            Outstate, Prob, Reward = movestate([i,j], [i+1, j])\n",
    "            newgrid[i, j] += Prob * (Reward + .9 * grid[Outstate[0], Outstate[1]])\n",
    "            # Plus Down\n",
    "            Outstate, Prob, Reward = movestate([i,j], [i-1, j])\n",
    "            newgrid[i, j] += Prob * (Reward + .9 * grid[Outstate[0], Outstate[1]])\n",
    "            # Plus Right\n",
    "            Outstate, Prob, Reward = movestate([i,j], [i, j+1])\n",
    "            newgrid[i, j] += Prob * (Reward + .9 * grid[Outstate[0], Outstate[1]])\n",
    "            # Plus Left\n",
    "            Outstate, Prob, Reward = movestate([i,j], [i, j-1])\n",
    "            newgrid[i, j] += Prob * (Reward + .9 * grid[Outstate[0], Outstate[1]])\n",
    "    #continue to loop until we converge\n",
    "    if np.sum(np.abs(grid - newgrid)) < 1e-4:\n",
    "        print(np.round(newgrid,1))\n",
    "        break\n",
    "    grid = newgrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridworld - Optimal Policy\n",
    "\n",
    "Next we evaluate the optimal policy and recreate figure 3.8 (pg. 73).\n",
    "\n",
    "![alt text](pic2.png)\n",
    "\n",
    "    \n",
    "Here the system dynamics are the same, but instead of taking the expected reward of each state we deterministically move to the state with the highest expected reward. The figure on the right is the optimal movement policy, while the center figure is the optimal policy's expected reward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 22.   24.4  22.   19.4  17.5]\n",
      " [ 19.8  22.   19.8  17.8  16. ]\n",
      " [ 17.8  19.8  17.8  16.   14.4]\n",
      " [ 16.   17.8  16.   14.4  13. ]\n",
      " [ 14.4  16.   14.4  13.   11.7]]\n"
     ]
    }
   ],
   "source": [
    "grid = np.zeros((5,5))   \n",
    "\n",
    "while True:\n",
    "    \n",
    "    newgrid = np.zeros((5,5))  \n",
    "\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            value = []\n",
    "            \n",
    "            Outstate, Prob, Reward = movestate([i,j], [i+1, j])\n",
    "            value.append(Reward + .9 * grid[Outstate[0], Outstate[1]])\n",
    "            \n",
    "            Outstate, Prob, Reward = movestate([i,j], [i-1, j])\n",
    "            value.append(Reward + .9 * grid[Outstate[0], Outstate[1]])\n",
    "            \n",
    "            Outstate, Prob, Reward = movestate([i,j], [i, j+1])\n",
    "            value.append(Reward + .9 * grid[Outstate[0], Outstate[1]])\n",
    "            \n",
    "            Outstate, Prob, Reward = movestate([i,j], [i, j-1])\n",
    "            value.append(Reward + .9 * grid[Outstate[0], Outstate[1]])\n",
    "            \n",
    "            newgrid[i,j] = np.max(value)\n",
    "            \n",
    "    if np.sum(np.abs(grid - newgrid)) < 1e-4:\n",
    "        print(np.round(newgrid, 1))\n",
    "        break\n",
    "    grid = newgrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter Highlights\n",
    "\n",
    "1. At time $t$, the agent receives a state $S \\in \\mathcal{S}$, performs an action $A_{t} \\in \\mathcal{A}(S_{t})$, and receives some reward $R_{t+1} \\in \\mathcal{R} \\subset \\mathbb{R}$, then transitions to a new state$ S_{t+1}$.\n",
    "\n",
    "2. At each step, the agent selects a \"next action\" $a$ based on a policy $\\pi_{t}$, defined as $\\pi_{t}(a \\mid s)$, which is the probability that a specific action is taken based on the current state $s$.\n",
    "\n",
    "3. With each timestep, the agent intends to maximize a goal, or reward. This reward communicates *what* you want achieved, not necessarily *how* you want it achieved. \n",
    "\n",
    "4. We define the goal as a miximization of the expected return $G_{t}$, which is defined as a function of the sum of the future rewards discounted by some *discount rate* $\\gamma$, where $0 \\leq \\gamma \\leq 1$.\n",
    "\n",
    "5. Formally, we define the *discounted return* to be $G_{t} = R_{t+1} + R_{t+2} + ... = \\sum\\limits_{k=0}^\\infty \\gamma^{k}R_{t+k+1} $\n",
    "\n",
    " - It follows that if $\\gamma < 1$ and the rewards are bounded, this sum will converge to a finite value on an infinite horizon.\n",
    "\n",
    "6. A process is considered *markovian* if the future states and actions of the process at time $t$ are dependent only on the state and actions of time $t-1$. Or, more formally, $p(s',r \\mid s,a) =  $ Pr$\\left\\{ S_{t+1} = s', R_{t+1} = r \\mid S_{t} = s, A_{t} = a \\right\\}$. This is important because it implies that the present state\\reward\\action tuples are in no way dependent on previous values of the same variables. \n",
    "\n",
    "7. A value function, $v_{\\pi}(s)$ is used to estimate *how good* it is for the agent to be in the given state, and is defined formally as $v_{\\pi}(s) = \\mathbb{E}_{\\pi} \\big[ G_{t} \\mid S_{t} = s \\big]$\n",
    "\n",
    "8. The action value function is used to estimate the vlaue of taking a particular action $a$ in state $s$ given policy $\\pi$ as $v_{\\pi}(s,a) = \\mathbb{E}_{\\pi} \\big[ G_{t} \\mid S_{t} = s, A_{t} = a \\big]$\n",
    "\n",
    "9. The value of a current state in relation to the value of its possible future successor states can be defined through the bellman equation, which is defined for the value function as $v_{\\pi}(s) = \\sum\\limits_{a} \\pi(a \\mid s) \\sum\\limits_{s',r} p(s',r \\mid s,a) \\big[ r + \\gamma v_{\\pi}(s')\\big]$. \n",
    " \n",
    " - This can be described in words as finding the current reward plus the discounted future reward (one step ahead), weighing it by the probability of that particular outcome occurring, the summing over all the possibilities of all the different outcomes.\n",
    " \n",
    " - This same equation also maximizes the action-value function described above.\n",
    " \n",
    "10. in a reinforcement learning task, we seek to maximise the return over a period of time, or the value over a period of time, on a particular policy $\\pi(A \\mid s)$\n",
    "\n",
    " - The *optimal value function* is defined as $v_{*}(s) = \\underset{\\pi}{\\operatorname{max}} v_{\\pi}(s,a)$\n",
    " \n",
    "11. Optimal policies share the same *optimal action-value function*, which is defined as $q_{*}(s,a) = \\underset{\\pi}{\\operatorname{max}} q_{\\pi}(s,a)$ and gives the expected return for taking action $a$ instate $s$ on policy $\\pi$\n",
    "\n",
    "12. The key difference to note between the average bellman equations above and the optimal bellman equations are the max operator, which instead seeks to maximize the expected return instead of simply taking an average over all the different possible transition states. This results in a deterministic transition as opposed to a probabilistic one, therefore we remove the expectation notation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
