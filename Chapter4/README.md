### [Dynamic Programming](http://people.tamu.edu/~levimcclenny/project/reinforcement-learning/Barto_Sutton_RL/Chapter4.html)
This chapter offers some insights into some popular, albeit fairly basic, dynamic programming algorithms used to evaluate MDPs when all the state-action interactions are known. Hence, in this chapter, we know all the transition probabilities, the rewards, etc, and there isnt anything to really "figure out." However, dont minsonctrue that description with a lack of aplication of these algorithms, as you can see in the [Jupyter Notebook](http://people.tamu.edu/~levimcclenny/project/reinforcement-learning/Barto_Sutton_RL/Chapter4.html) and in the text for this chapter, we can apply these algorithms to real-life problems and handle some subtle non-linearities in problems that traditional optimization algorithms might struggle with. This, the efficacy of these algorithms, and hence their presentation in this chapter, is something to be valued. 
